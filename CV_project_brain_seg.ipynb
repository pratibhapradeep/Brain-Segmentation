{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratibhapradeep/Brain-Segmentation/blob/main/CV_project_brain_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, jaccard_score\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the correct path to your BraTS data\n",
        "data_path = '/content/drive/MyDrive/original_data'\n",
        "print(f\"Data path: {data_path}\")\n",
        "print(f\"Available directories: {sorted(os.listdir(data_path))}\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install nibabel scikit-image tqdm pandas matplotlib opencv-python seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D0q9-H6_Q-i",
        "outputId": "b555f2b5-f6f3-4474-fa87-6cc887cd0d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data path: /content/drive/MyDrive/original_data\n",
            "Available directories: ['BraTS20_Processed_Training_001', 'BraTS20_Processed_Training_002', 'BraTS20_Processed_Training_003', 'BraTS20_Processed_Training_004', 'BraTS20_Processed_Training_005', 'BraTS20_Processed_Training_006', 'BraTS20_Processed_Training_007']\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.13.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNiaGgd-_CJl"
      },
      "outputs": [],
      "source": [
        "# Define the Brain Tumor Segmentation Model based on Rastogi et al. paper\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n"
      ],
      "metadata": {
        "id": "p6TjZq1B_gZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Adjust dimensions if necessary\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "5EhFqu45_iwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "otvqQmCH_mNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainTumorSegmentationNet(nn.Module):\n",
        "    def __init__(self, n_channels=4, n_classes=1, bilinear=True):\n",
        "        super(BrainTumorSegmentationNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return torch.sigmoid(logits)"
      ],
      "metadata": {
        "id": "3eKOw61x_n4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        batch_size = predicted.size(0)\n",
        "\n",
        "        # Flatten\n",
        "        pred_flat = predicted.view(batch_size, -1)\n",
        "        target_flat = target.view(batch_size, -1)\n",
        "\n",
        "        # Calculate Dice\n",
        "        intersection = (pred_flat * target_flat).sum(1)\n",
        "        union = pred_flat.sum(1) + target_flat.sum(1)\n",
        "\n",
        "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
        "\n",
        "        return 1 - dice.mean()\n"
      ],
      "metadata": {
        "id": "QUaXW2sa_qqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.8, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce_loss = nn.BCELoss(reduction='none')\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        bce = self.bce_loss(predicted, target)\n",
        "\n",
        "        # Apply focal weighting\n",
        "        pt = torch.exp(-bce)\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Apply alpha for class imbalance\n",
        "        if self.alpha is not None:\n",
        "            focal_weight = self.alpha * focal_weight\n",
        "\n",
        "        return (focal_weight * bce).mean()"
      ],
      "metadata": {
        "id": "G1SyB_0zMNEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.focal_loss = FocalLoss()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        dice = self.dice_loss(predicted, target)\n",
        "        focal = self.focal_loss(predicted, target)\n",
        "        return self.dice_weight * dice + self.focal_weight * focal"
      ],
      "metadata": {
        "id": "BDCQA_ZfMP3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for calculating Dice coefficient\n",
        "def dice_coefficient(pred, target):\n",
        "    smooth = 1.0\n",
        "\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    union = pred_flat.sum() + target_flat.sum()\n",
        "\n",
        "    return (2. * intersection + smooth) / (union + smooth)"
      ],
      "metadata": {
        "id": "h5wk3_Sz_sfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for calculating IoU (Jaccard index)\n",
        "def iou_coefficient(pred, target):\n",
        "    smooth = 1.0\n",
        "\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    union = pred_flat.sum() + target_flat.sum() - intersection\n",
        "\n",
        "    return (intersection + smooth) / (union + smooth)"
      ],
      "metadata": {
        "id": "NRDLC5RKMU7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for BraTS\n",
        "class BraTSDataset(Dataset):\n",
        "    def __init__(self, root_dir, slice_range=(30, 120), slice_step=3):\n",
        "        self.root_dir = root_dir\n",
        "        self.slice_range = slice_range\n",
        "        self.slice_step = slice_step\n",
        "        self.samples = []\n",
        "\n",
        "        # Get all directories (each contains one patient data)\n",
        "        self.folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f)) and \"Training\" in f]\n",
        "\n",
        "        print(f\"Found {len(self.folders)} training folders\")\n",
        "\n",
        "        # Explore each folder to find the MRI modalities\n",
        "        for folder in tqdm(self.folders, desc=\"Scanning folders\"):\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "\n",
        "            # Find the files for each modality\n",
        "            t1_file = None\n",
        "            t1ce_file = None\n",
        "            t2_file = None\n",
        "            flair_file = None\n",
        "            seg_file = None\n",
        "\n",
        "            # Search for the MRI files in each patient folder\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith('.nii.gz') or file.endswith('.nii'):\n",
        "                    if 't1.' in file.lower() or '_t1_' in file.lower() or '_t1.' in file.lower():\n",
        "                        t1_file = os.path.join(folder_path, file)\n",
        "                    elif 't1ce' in file.lower() or 't1c.' in file.lower() or 't1_ce' in file.lower():\n",
        "                        t1ce_file = os.path.join(folder_path, file)\n",
        "                    elif 't2.' in file.lower() or '_t2_' in file.lower() or '_t2.' in file.lower():\n",
        "                        t2_file = os.path.join(folder_path, file)\n",
        "                    elif 'flair' in file.lower():\n",
        "                        flair_file = os.path.join(folder_path, file)\n",
        "                    elif 'seg' in file.lower() or 'mask' in file.lower():\n",
        "                        seg_file = os.path.join(folder_path, file)\n",
        "\n",
        "            # Check if we found all modalities\n",
        "            if all([t1_file, t1ce_file, t2_file, flair_file, seg_file]):\n",
        "                # Load and get dimensions\n",
        "                img = nib.load(t1_file)\n",
        "                n_slices = img.shape[2]  # Number of slices in the 3D volume\n",
        "\n",
        "                # Select slices in the specified range with the given step\n",
        "                min_slice = max(0, slice_range[0])\n",
        "                max_slice = min(n_slices, slice_range[1])\n",
        "\n",
        "                for slice_idx in range(min_slice, max_slice, slice_step):\n",
        "                    self.samples.append({\n",
        "                        'folder': folder,\n",
        "                        'slice_idx': slice_idx,\n",
        "                        't1_file': t1_file,\n",
        "                        't1ce_file': t1ce_file,\n",
        "                        't2_file': t2_file,\n",
        "                        'flair_file': flair_file,\n",
        "                        'seg_file': seg_file\n",
        "                    })\n",
        "            else:\n",
        "                missing = []\n",
        "                if not t1_file: missing.append(\"T1\")\n",
        "                if not t1ce_file: missing.append(\"T1ce\")\n",
        "                if not t2_file: missing.append(\"T2\")\n",
        "                if not flair_file: missing.append(\"FLAIR\")\n",
        "                if not seg_file: missing.append(\"Segmentation\")\n",
        "                print(f\"Missing modalities in {folder}: {', '.join(missing)}\")\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} slices from {len(self.folders)} folders\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load each modality\n",
        "        t1_slice = self._load_slice(sample['t1_file'], sample['slice_idx'])\n",
        "        t1ce_slice = self._load_slice(sample['t1ce_file'], sample['slice_idx'])\n",
        "        t2_slice = self._load_slice(sample['t2_file'], sample['slice_idx'])\n",
        "        flair_slice = self._load_slice(sample['flair_file'], sample['slice_idx'])\n",
        "\n",
        "        # Load segmentation mask\n",
        "        seg_slice = self._load_slice(sample['seg_file'], sample['slice_idx'], is_mask=True)\n",
        "\n",
        "        # Combine all modalities into a 4-channel input\n",
        "        image = np.stack([t1_slice, t1ce_slice, t2_slice, flair_slice], axis=0)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        image_tensor = torch.from_numpy(image).float()\n",
        "        mask_tensor = torch.from_numpy(seg_slice).float().unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        return {\n",
        "            'image': image_tensor,\n",
        "            'mask': mask_tensor,\n",
        "            'folder': sample['folder'],\n",
        "            'slice_idx': sample['slice_idx']\n",
        "        }\n",
        "\n",
        "    def _load_slice(self, file_path, slice_idx, is_mask=False):\n",
        "        \"\"\"Load a single slice from a NIfTI file and normalize\"\"\"\n",
        "        img = nib.load(file_path)\n",
        "        data = img.get_fdata()\n",
        "        slice_data = data[:, :, slice_idx]\n",
        "\n",
        "        if is_mask:\n",
        "            # Binary mask: any non-zero value is considered tumor\n",
        "            return (slice_data > 0).astype(np.float32)\n",
        "        else:\n",
        "            # Normalize image to [0,1]\n",
        "            min_val = np.min(slice_data)\n",
        "            max_val = np.max(slice_data)\n",
        "\n",
        "            if max_val > min_val:\n",
        "                normalized = (slice_data - min_val) / (max_val - min_val)\n",
        "                return normalized\n",
        "            else:\n",
        "                return np.zeros_like(slice_data, dtype=np.float32)"
      ],
      "metadata": {
        "id": "ugYZLkNe_ufE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with tqdm(dataloader, desc=\"Training\") as pbar:\n",
        "        for batch in pbar:\n",
        "            images = batch['image'].to(device)\n",
        "            masks = batch['mask'].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "YtjuDbxX_1_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function with detailed metrics\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_dice = 0\n",
        "    val_iou = 0\n",
        "    val_precision = 0\n",
        "    val_recall = 0\n",
        "    val_f1 = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(dataloader, desc=\"Validation\") as pbar:\n",
        "            for batch in pbar:\n",
        "                images = batch['image'].to(device)\n",
        "                masks = batch['mask'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate metrics\n",
        "                pred_masks = (outputs > 0.5).float()\n",
        "\n",
        "                # Dice coefficient\n",
        "                dice = dice_coefficient(pred_masks, masks)\n",
        "                val_dice += dice.item()\n",
        "\n",
        "                # IoU (Jaccard index)\n",
        "                iou = iou_coefficient(pred_masks, masks)\n",
        "                val_iou += iou.item()\n",
        "\n",
        "                # Move to CPU for sklearn metrics\n",
        "                pred_np = pred_masks.cpu().numpy().flatten()\n",
        "                target_np = masks.cpu().numpy().flatten()\n",
        "\n",
        "                # Store for confusion matrix\n",
        "                all_preds.extend(pred_np)\n",
        "                all_targets.extend(target_np)\n",
        "\n",
        "                # Calculate precision, recall, F1\n",
        "                if np.sum(target_np) > 0:  # Skip slices with no tumor\n",
        "                    val_precision += precision_score(target_np, pred_np, zero_division=1)\n",
        "                    val_recall += recall_score(target_np, pred_np, zero_division=1)\n",
        "                    val_f1 += f1_score(target_np, pred_np, zero_division=1)\n",
        "\n",
        "                pbar.set_postfix(loss=loss.item(), dice=dice.item())\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    metrics = {\n",
        "        'loss': val_loss / len(dataloader),\n",
        "        'dice': val_dice / len(dataloader),\n",
        "        'iou': val_iou / len(dataloader),\n",
        "        'precision': val_precision / len(dataloader),\n",
        "        'recall': val_recall / len(dataloader),\n",
        "        'f1': val_f1 / len(dataloader),\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Cs_3Zf7G_5tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function with multiple visualizations\n",
        "def visualize_predictions(model, val_loader, device, num_samples=5):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch\n",
        "    batch = next(iter(val_loader))\n",
        "    images = batch['image'].to(device)\n",
        "    masks = batch['mask'].to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "    # Convert to numpy\n",
        "    images_np = images.cpu().numpy()\n",
        "    masks_np = masks.cpu().numpy()\n",
        "    outputs_np = outputs.cpu().numpy()\n",
        "    preds_np = preds.cpu().numpy()\n",
        "\n",
        "    # 1. Basic prediction visualization\n",
        "    fig1, axes = plt.subplots(min(num_samples, len(images)), 5, figsize=(20, 4*min(num_samples, len(images))))\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        # T1 image\n",
        "        axes[i, 0].imshow(images_np[i, 0], cmap='gray')\n",
        "        axes[i, 0].set_title(\"T1\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # FLAIR image\n",
        "        axes[i, 1].imshow(images_np[i, 3], cmap='gray')\n",
        "        axes[i, 1].set_title(\"FLAIR\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Ground truth\n",
        "        axes[i, 2].imshow(masks_np[i, 0], cmap='gray')\n",
        "        axes[i, 2].set_title(\"Ground Truth\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # Probability map\n",
        "        im = axes[i, 3].imshow(outputs_np[i, 0], cmap='jet', vmin=0, vmax=1)\n",
        "        axes[i, 3].set_title(\"Probability\")\n",
        "        axes[i, 3].axis('off')\n",
        "        plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # Binary prediction\n",
        "        axes[i, 4].imshow(preds_np[i, 0], cmap='gray')\n",
        "        axes[i, 4].set_title(\"Prediction\")\n",
        "        axes[i, 4].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 2. Overlay visualization\n",
        "    fig2, axes = plt.subplots(min(num_samples, len(images)), 3, figsize=(15, 4*min(num_samples, len(images))))\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        # Original FLAIR image\n",
        "        axes[i, 0].imshow(images_np[i, 3], cmap='gray')\n",
        "        axes[i, 0].set_title(\"FLAIR Image\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Ground truth overlay\n",
        "        axes[i, 1].imshow(images_np[i, 3], cmap='gray')\n",
        "        mask_overlay = np.ma.masked_where(masks_np[i, 0] < 0.5, masks_np[i, 0])\n",
        "        axes[i, 1].imshow(mask_overlay, cmap='autumn', alpha=0.7)\n",
        "        axes[i, 1].set_title(\"Ground Truth Overlay\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Prediction overlay\n",
        "        axes[i, 2].imshow(images_np[i, 3], cmap='gray')\n",
        "        pred_overlay = np.ma.masked_where(preds_np[i, 0] < 0.5, preds_np[i, 0])\n",
        "        axes[i, 2].imshow(pred_overlay, cmap='cool', alpha=0.7)\n",
        "        axes[i, 2].set_title(\"Prediction Overlay\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 3. Error analysis visualization\n",
        "    fig3, axes = plt.subplots(min(num_samples, len(images)), 3, figsize=(15, 4*min(num_samples, len(images))))\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        # Original FLAIR image\n",
        "        axes[i, 0].imshow(images_np[i, 3], cmap='gray')\n",
        "        axes[i, 0].set_title(\"FLAIR Image\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # True Positive (green) and False Negative (red)\n",
        "        tp = np.logical_and(preds_np[i, 0] > 0.5, masks_np[i, 0] > 0.5)\n",
        "        fn = np.logical_and(preds_np[i, 0] <= 0.5, masks_np[i, 0] > 0.5)\n",
        "\n",
        "        axes[i, 1].imshow(images_np[i, 3], cmap='gray')\n",
        "        axes[i, 1].imshow(np.ma.masked_where(tp == 0, tp), cmap='Greens', alpha=0.7)\n",
        "        axes[i, 1].imshow(np.ma.masked_where(fn == 0, fn), cmap='Reds', alpha=0.7)\n",
        "        axes[i, 1].set_title(\"TP (green) & FN (red)\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # True Negative (black) and False Positive (blue)\n",
        "        tn = np.logical_and(preds_np[i, 0] <= 0.5, masks_np[i, 0] <= 0.5)\n",
        "        fp = np.logical_and(preds_np[i, 0] > 0.5, masks_np[i, 0] <= 0.5)\n",
        "\n",
        "        axes[i, 2].imshow(images_np[i, 3], cmap='gray')\n",
        "        # We don't visualize true negatives as they're the majority of the image\n",
        "        axes[i, 2].imshow(np.ma.masked_where(fp == 0, fp), cmap='Blues', alpha=0.7)\n",
        "        axes[i, 2].set_title(\"FP (blue)\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig1, fig2, fig3\n"
      ],
      "metadata": {
        "id": "CB3txQLG_73p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training curves with multiple metrics\n",
        "def plot_training_curves(metrics_df, output_dir):\n",
        "    # 1. Loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], 'b-', label='Training Loss')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], 'r-', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'loss_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Dice and IoU curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_dice'], 'g-', label='Dice Coefficient')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_iou'], 'y-', label='IoU (Jaccard)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation Dice and IoU Scores')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'dice_iou_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Precision, Recall, F1 curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_precision'], 'm-', label='Precision')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_recall'], 'c-', label='Recall')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_f1'], 'k-', label='F1 Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation Precision, Recall, and F1 Scores')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'precision_recall_f1_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(cm, output_dir):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=['No Tumor', 'Tumor'],\n",
        "                yticklabels=['No Tumor', 'Tumor'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "okDXQHeRNRqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "def get_data_loaders(root_dir, batch_size=8, val_split=0.2, slice_range=(30, 120), slice_step=3, num_workers=2):\n",
        "    # Create dataset\n",
        "    dataset = BraTSDataset(root_dir, slice_range=slice_range, slice_step=slice_step)\n",
        "\n",
        "    # Check if dataset is empty\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(f\"No valid data found in {root_dir}. Check paths and file names.\")\n",
        "\n",
        "    # Split into train/val datasets\n",
        "    train_size = int((1 - val_split) * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "QSBnJPde__2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training function with different optimizers\n",
        "def train_model(data_path, output_dir='model_output', batch_size=4, num_epochs=30,\n",
        "               slice_step=3, optimizer_name='adam', learning_rate=1e-4, loss_type='dice'):\n",
        "    # Create experiment directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    exp_name = f\"{optimizer_name}_lr{learning_rate}_{loss_type}_{timestamp}\"\n",
        "    output_dir = os.path.join(output_dir, exp_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set device (GPU if available)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader = get_data_loaders(\n",
        "        data_path,\n",
        "        batch_size=batch_size,\n",
        "        slice_step=slice_step\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_loader.dataset)} slices\")\n",
        "    print(f\"Validation set: {len(val_loader.dataset)} slices\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = BrainTumorSegmentationNet(n_channels=4, n_classes=1)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize loss function based on type\n",
        "    if loss_type == 'dice':\n",
        "        criterion = DiceLoss()\n",
        "    elif loss_type == 'focal':\n",
        "        criterion = FocalLoss()\n",
        "    elif loss_type == 'combined':\n",
        "        criterion = CombinedLoss()\n",
        "    else:\n",
        "        criterion = DiceLoss()  # Default\n",
        "\n",
        "    # Initialize optimizer based on name\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Default\n",
        "\n",
        "    # Initialize learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Track metrics\n",
        "    train_losses = []\n",
        "    val_metrics_list = []\n",
        "    best_val_dice = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "     # Save configuration\n",
        "    config = {\n",
        "        'data_path': data_path,\n",
        "        'optimizer': optimizer_name,\n",
        "        'learning_rate': learning_rate,\n",
        "        'loss_function': loss_type,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': num_epochs,\n",
        "        'slice_step': slice_step,\n",
        "        'device': str(device),\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "    # Save config to JSON\n",
        "    import json\n",
        "    with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    metrics_df = pd.DataFrame()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Validate with detailed metrics\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n",
        "              f\"Val Dice: {val_metrics['dice']:.4f} | Val IoU: {val_metrics['iou']:.4f}\")\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_metrics_list.append(val_metrics)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['dice'] > best_val_dice:\n",
        "            best_val_dice = val_metrics['dice']\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pth'))\n",
        "            print(f\"Saved new best model with Dice: {val_metrics['dice']:.4f}\")\n",
        "\n",
        "            # Save confusion matrix for best model\n",
        "            plot_confusion_matrix(val_metrics['confusion_matrix'], output_dir)\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_metrics': val_metrics\n",
        "            }, os.path.join(output_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
        "\n",
        "        # Collect all metrics for DataFrame\n",
        "        epoch_metrics = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_metrics['loss'],\n",
        "            'val_dice': val_metrics['dice'],\n",
        "            'val_iou': val_metrics['iou'],\n",
        "            'val_precision': val_metrics['precision'],\n",
        "            'val_recall': val_metrics['recall'],\n",
        "            'val_f1': val_metrics['f1'],\n",
        "            'learning_rate': optimizer.param_groups[0]['lr']\n",
        "        }\n",
        "\n",
        "        # Append to DataFrame\n",
        "        metrics_df = pd.concat([metrics_df, pd.DataFrame([epoch_metrics])], ignore_index=True)\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pth'))\n",
        "\n",
        "    # Training time\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {training_time / 60:.2f} minutes\")\n",
        "    print(f\"Best validation Dice: {best_val_dice:.4f} at epoch {best_epoch}\")\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df.to_csv(os.path.join(output_dir, 'training_metrics.csv'), index=False)\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_training_curves(metrics_df, output_dir)\n",
        "\n",
        "    # Visualize predictions\n",
        "    print(\"Generating visualizations...\")\n",
        "    model.load_state_dict(torch.load(os.path.join(output_dir, 'best_model.pth')))\n",
        "    fig1, fig2, fig3 = visualize_predictions(model, val_loader, device)\n",
        "    fig1.savefig(os.path.join(output_dir, 'basic_predictions.png'))\n",
        "    fig2.savefig(os.path.join(output_dir, 'overlay_predictions.png'))\n",
        "    fig3.savefig(os.path.join(output_dir, 'error_analysis.png'))\n",
        "\n",
        "    plt.close(fig1)\n",
        "    plt.close(fig2)\n",
        "    plt.close(fig3)\n",
        "\n",
        "    # Generate detailed test report\n",
        "    test_report = {\n",
        "        'experiment_name': exp_name,\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_dice': best_val_dice,\n",
        "        'final_metrics': {k: v for k, v in val_metrics.items() if k != 'confusion_matrix'},\n",
        "        'training_time_minutes': training_time / 60,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, 'test_report.json'), 'w') as f:\n",
        "        json.dump(test_report, f, indent=4)\n",
        "\n",
        "    return model, metrics_df, output_dir\n",
        "\n"
      ],
      "metadata": {
        "id": "KpYAHBi4AD32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run multiple experiments with different configurations\n",
        "def run_experiments(data_path, base_output_dir):\n",
        "    # Define experiment configurations\n",
        "    experiments = [\n",
        "        # Different optimizers\n",
        "        {'optimizer_name': 'adam', 'learning_rate': 1e-4, 'loss_type': 'dice', 'num_epochs': 10},\n",
        "        {'optimizer_name': 'sgd', 'learning_rate': 1e-3, 'loss_type': 'dice', 'num_epochs': 10},\n",
        "        {'optimizer_name': 'rmsprop', 'learning_rate': 5e-4, 'loss_type': 'dice', 'num_epochs': 10},\n",
        "\n",
        "        # Different loss functions\n",
        "        {'optimizer_name': 'adam', 'learning_rate': 1e-4, 'loss_type': 'focal', 'num_epochs': 10},\n",
        "        {'optimizer_name': 'adam', 'learning_rate': 1e-4, 'loss_type': 'combined', 'num_epochs': 10},\n",
        "\n",
        "        # Different learning rates\n",
        "        {'optimizer_name': 'adam', 'learning_rate': 5e-5, 'loss_type': 'dice', 'num_epochs': 10},\n",
        "        {'optimizer_name': 'adam', 'learning_rate': 5e-4, 'loss_type': 'dice', 'num_epochs': 10},\n",
        "    ]\n",
        "\n",
        "    # Create results directory\n",
        "    results_dir = os.path.join(base_output_dir, f'experiments_{datetime.now().strftime(\"%Y%m%d\")}')\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Run each experiment\n",
        "    experiment_results = []\n",
        "\n",
        "    for i, exp_config in enumerate(experiments):\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"Running experiment {i+1}/{len(experiments)}: {exp_config}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Train model with this configuration\n",
        "        model, metrics, output_dir = train_model(\n",
        "            data_path=data_path,\n",
        "            output_dir=results_dir,\n",
        "            batch_size=4,\n",
        "            **exp_config\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        best_dice = metrics['val_dice'].max()\n",
        "        best_epoch = metrics['val_dice'].argmax() + 1\n",
        "\n",
        "        experiment_results.append({\n",
        "            'experiment_id': i+1,\n",
        "            'config': exp_config,\n",
        "            'best_dice': best_dice,\n",
        "            'best_epoch': best_epoch,\n",
        "            'output_dir': output_dir\n",
        "        })\n",
        "\n",
        "        print(f\"\\nExperiment {i+1} completed with best Dice: {best_dice:.4f} at epoch {best_epoch}\")\n",
        "\n",
        "    # Create comparison report\n",
        "    comparison_df = pd.DataFrame([\n",
        "        {\n",
        "            'Experiment': i+1,\n",
        "            'Optimizer': exp['config']['optimizer_name'],\n",
        "            'Learning Rate': exp['config']['learning_rate'],\n",
        "            'Loss Function': exp['config']['loss_type'],\n",
        "            'Best Dice': exp['best_dice'],\n",
        "            'Best Epoch': exp['best_epoch']\n",
        "        }\n",
        "        for i, exp in enumerate(experiment_results)\n",
        "    ])\n",
        "\n",
        "    # Save comparison report\n",
        "    comparison_df.to_csv(os.path.join(results_dir, 'experiment_comparison.csv'), index=False)\n",
        "\n",
        "    # Plot comparison bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Create experiment labels\n",
        "    exp_labels = [\n",
        "        f\"{exp['config']['optimizer_name']}\\n\"\n",
        "        f\"lr={exp['config']['learning_rate']}\\n\"\n",
        "        f\"{exp['config']['loss_type']}\"\n",
        "        for exp in experiment_results\n",
        "    ]\n",
        "\n",
        "    # Plot Dice scores\n",
        "    dice_scores = [exp['best_dice'] for exp in experiment_results]\n",
        "    bars = plt.bar(exp_labels, dice_scores, color='skyblue')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, score in zip(bars, dice_scores):\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width()/2,\n",
        "            bar.get_height() + 0.01,\n",
        "            f\"{score:.4f}\",\n",
        "            ha='center'\n",
        "        )\n",
        "\n",
        "    plt.ylabel('Best Dice Coefficient')\n",
        "    plt.title('Comparison of Experiment Configurations')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(results_dir, 'experiment_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nAll experiments completed! Results saved to {results_dir}\")\n",
        "    return comparison_df, experiment_results\n"
      ],
      "metadata": {
        "id": "mSSskjcrN8X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code with either a single model or multiple experiments\n",
        "if __name__ == \"__main__\":\n",
        "    # Set parameters\n",
        "    batch_size = 4          # Adjust based on GPU memory\n",
        "    num_epochs = 10         # Default epochs for training\n",
        "    slice_step = 3          # Use every third slice to save memory\n",
        "\n",
        "    \"\"\"\n",
        "    Either train a single model...\n",
        "    model, metrics_df, output_dir = train_model(\n",
        "        data_path=data_path,  # This is set at the top of the script\n",
        "        output_dir='/content/drive/MyDrive/brain_tumor_model_output',\n",
        "        batch_size=batch_size,\n",
        "        num_epochs=num_epochs,\n",
        "        slice_step=slice_step,\n",
        "        optimizer_name='adam',  # 'adam', 'sgd', or 'rmsprop'\n",
        "        learning_rate=1e-4,\n",
        "        loss_type='dice'        # 'dice', 'focal', or 'combined'\n",
        "    )\n",
        "\n",
        "    print(f\"Single model training complete! Results saved to {output_dir}\")\n",
        "\n",
        "    \"\"\"\n",
        "    # Or uncomment to run multiple experiments\n",
        "\n",
        "    comparison_df, experiment_results = run_experiments(\n",
        "        data_path=data_path,\n",
        "        base_output_dir='/content/drive/MyDrive/brain_tumor_model_output'\n",
        "    )\n",
        "\n",
        "    print(\"Multiple experiments complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcvwCtlBAKKN",
        "outputId": "bf99e4ae-48a9-4b7d-f8da-1918d68e9060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 1/7: {'optimizer_name': 'adam', 'learning_rate': 0.0001, 'loss_type': 'dice', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 105.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:47<00:00,  5.41s/it, loss=0.818]\n",
            "Validation: 100%|| 11/11 [00:20<00:00,  1.89s/it, dice=4.49e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8713 | Val Loss: 0.8901 | Val Dice: 0.1198 | Val IoU: 0.0695\n",
            "Saved new best model with Dice: 0.1198\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.98s/it, loss=0.82]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.59s/it, dice=0.000137, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8166 | Val Loss: 0.8665 | Val Dice: 0.0769 | Val IoU: 0.0409\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:33<00:00,  5.08s/it, loss=0.712]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.63s/it, dice=1, loss=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7996 | Val Loss: 0.9156 | Val Dice: 0.2043 | Val IoU: 0.1573\n",
            "Saved new best model with Dice: 0.2043\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:35<00:00,  5.13s/it, loss=0.883]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000293, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7947 | Val Loss: 0.7901 | Val Dice: 0.3160 | Val IoU: 0.2248\n",
            "Saved new best model with Dice: 0.3160\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:31<00:00,  5.03s/it, loss=0.78]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=0.000297, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7801 | Val Loss: 0.7882 | Val Dice: 0.4516 | Val IoU: 0.3414\n",
            "Saved new best model with Dice: 0.4516\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:35<00:00,  5.12s/it, loss=0.689]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.62s/it, dice=0.000394, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7668 | Val Loss: 0.8077 | Val Dice: 0.3796 | Val IoU: 0.2750\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:32<00:00,  5.06s/it, loss=0.738]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.65s/it, dice=0.000173, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7662 | Val Loss: 0.8103 | Val Dice: 0.2902 | Val IoU: 0.1996\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:32<00:00,  5.07s/it, loss=0.77]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.65s/it, dice=0.000701, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7394 | Val Loss: 0.7360 | Val Dice: 0.3966 | Val IoU: 0.2933\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.96s/it, loss=0.493]\n",
            "Validation: 100%|| 11/11 [00:16<00:00,  1.55s/it, dice=0.000948, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7345 | Val Loss: 0.7528 | Val Dice: 0.3393 | Val IoU: 0.2305\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.88s/it, loss=0.753]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.74s/it, dice=0.0027, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7287 | Val Loss: 0.7638 | Val Dice: 0.4381 | Val IoU: 0.3299\n",
            "\n",
            "Training completed in 38.69 minutes\n",
            "Best validation Dice: 0.4516 at epoch 5\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 1 completed with best Dice: 0.4516 at epoch 5\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 2/7: {'optimizer_name': 'sgd', 'learning_rate': 0.001, 'loss_type': 'dice', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 117.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.96s/it, loss=0.932]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.64s/it, dice=0.00016, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9173 | Val Loss: 0.9047 | Val Dice: 0.2223 | Val IoU: 0.1380\n",
            "Saved new best model with Dice: 0.2223\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.88s/it, loss=0.794]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000166, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8893 | Val Loss: 0.8872 | Val Dice: 0.2140 | Val IoU: 0.1317\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.98s/it, loss=0.902]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.59s/it, dice=0.000187, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8767 | Val Loss: 0.8792 | Val Dice: 0.2309 | Val IoU: 0.1445\n",
            "Saved new best model with Dice: 0.2309\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:23<00:00,  4.86s/it, loss=0.765]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000223, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8634 | Val Loss: 0.8693 | Val Dice: 0.2504 | Val IoU: 0.1603\n",
            "Saved new best model with Dice: 0.2504\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:25<00:00,  4.88s/it, loss=0.974]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.55s/it, dice=0.000275, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8451 | Val Loss: 0.8579 | Val Dice: 0.2710 | Val IoU: 0.1781\n",
            "Saved new best model with Dice: 0.2710\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:22<00:00,  4.82s/it, loss=0.769]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.77s/it, dice=0.000411, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8216 | Val Loss: 0.8218 | Val Dice: 0.3160 | Val IoU: 0.2198\n",
            "Saved new best model with Dice: 0.3160\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:22<00:00,  4.82s/it, loss=0.57]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.58s/it, dice=0.000622, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7881 | Val Loss: 0.8088 | Val Dice: 0.2605 | Val IoU: 0.1730\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.86s/it, loss=0.681]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000353, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7520 | Val Loss: 0.7853 | Val Dice: 0.2424 | Val IoU: 0.1581\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.87s/it, loss=0.647]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.62s/it, dice=0.00746, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7312 | Val Loss: 0.7491 | Val Dice: 0.3130 | Val IoU: 0.2148\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:21<00:00,  4.80s/it, loss=0.548]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.71s/it, dice=0.00147, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7093 | Val Loss: 0.7324 | Val Dice: 0.3008 | Val IoU: 0.2066\n",
            "\n",
            "Training completed in 37.29 minutes\n",
            "Best validation Dice: 0.3160 at epoch 6\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 2 completed with best Dice: 0.3160 at epoch 6\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 3/7: {'optimizer_name': 'rmsprop', 'learning_rate': 0.0005, 'loss_type': 'dice', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 88.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:21<00:00,  4.79s/it, loss=0.956]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.73s/it, dice=0.000196, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8409 | Val Loss: 0.8593 | Val Dice: 0.2664 | Val IoU: 0.1725\n",
            "Saved new best model with Dice: 0.2664\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:21<00:00,  4.80s/it, loss=0.845]\n",
            "Validation: 100%|| 11/11 [00:22<00:00,  2.06s/it, dice=0.000833, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7683 | Val Loss: 0.8691 | Val Dice: 0.2585 | Val IoU: 0.1597\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:26<00:00,  4.91s/it, loss=0.781]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=0.00113, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7360 | Val Loss: 0.7470 | Val Dice: 0.5305 | Val IoU: 0.4208\n",
            "Saved new best model with Dice: 0.5305\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.96s/it, loss=0.59]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.64s/it, dice=0.00115, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7056 | Val Loss: 0.7311 | Val Dice: 0.5156 | Val IoU: 0.4059\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.94s/it, loss=0.897]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=0.00806, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6901 | Val Loss: 0.7553 | Val Dice: 0.4574 | Val IoU: 0.3479\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.86s/it, loss=0.797]\n",
            "Validation: 100%|| 11/11 [00:20<00:00,  1.84s/it, dice=0.000265, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6986 | Val Loss: 0.7877 | Val Dice: 0.3402 | Val IoU: 0.2368\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.86s/it, loss=0.617]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.58s/it, dice=0.00368, loss=0.995]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6816 | Val Loss: 0.7102 | Val Dice: 0.5988 | Val IoU: 0.4943\n",
            "Saved new best model with Dice: 0.5988\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:26<00:00,  4.91s/it, loss=0.593]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=0.000514, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6793 | Val Loss: 0.7464 | Val Dice: 0.4211 | Val IoU: 0.3086\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.87s/it, loss=0.246]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000359, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6868 | Val Loss: 0.7961 | Val Dice: 0.3152 | Val IoU: 0.2152\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:23<00:00,  4.84s/it, loss=0.812]\n",
            "Validation: 100%|| 11/11 [00:16<00:00,  1.51s/it, dice=0.0159, loss=0.988]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6761 | Val Loss: 0.7156 | Val Dice: 0.6277 | Val IoU: 0.5178\n",
            "Saved new best model with Dice: 0.6277\n",
            "\n",
            "Training completed in 37.33 minutes\n",
            "Best validation Dice: 0.6277 at epoch 10\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 3 completed with best Dice: 0.6277 at epoch 10\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 4/7: {'optimizer_name': 'adam', 'learning_rate': 0.0001, 'loss_type': 'focal', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 116.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.0313]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=1, loss=0.0387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0691 | Val Loss: 0.0429 | Val Dice: 0.6610 | Val IoU: 0.5657\n",
            "Saved new best model with Dice: 0.6610\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.0451]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.80s/it, dice=0.00116, loss=0.0327]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0357 | Val Loss: 0.0324 | Val Dice: 0.6245 | Val IoU: 0.5104\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.93s/it, loss=0.0211]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.62s/it, dice=0.0278, loss=0.0183]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0259 | Val Loss: 0.0210 | Val Dice: 0.8027 | Val IoU: 0.7243\n",
            "Saved new best model with Dice: 0.8027\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:31<00:00,  5.04s/it, loss=0.0179]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.59s/it, dice=1, loss=0.0149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0226 | Val Loss: 0.0179 | Val Dice: 0.9067 | Val IoU: 0.8377\n",
            "Saved new best model with Dice: 0.9067\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:30<00:00,  5.00s/it, loss=0.0175]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.61s/it, dice=0.00211, loss=0.0193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0186 | Val Loss: 0.0254 | Val Dice: 0.5880 | Val IoU: 0.4718\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:25<00:00,  4.89s/it, loss=0.0126]\n",
            "Validation: 100%|| 11/11 [00:22<00:00,  2.02s/it, dice=0.0526, loss=0.012]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0184 | Val Loss: 0.0151 | Val Dice: 0.8169 | Val IoU: 0.7437\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:30<00:00,  5.00s/it, loss=0.013]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.73s/it, dice=0.2, loss=0.008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0141 | Val Loss: 0.0107 | Val Dice: 0.8541 | Val IoU: 0.7950\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.97s/it, loss=0.0139]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=0.25, loss=0.00764]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0123 | Val Loss: 0.0110 | Val Dice: 0.8501 | Val IoU: 0.7853\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:32<00:00,  5.06s/it, loss=0.0151]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.63s/it, dice=1, loss=0.00687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0099 | Val Loss: 0.0088 | Val Dice: 0.8803 | Val IoU: 0.8307\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.00811]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.68s/it, dice=0.167, loss=0.00649]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0096 | Val Loss: 0.0086 | Val Dice: 0.7682 | Val IoU: 0.7094\n",
            "\n",
            "Training completed in 38.08 minutes\n",
            "Best validation Dice: 0.9067 at epoch 4\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 4 completed with best Dice: 0.9067 at epoch 4\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 5/7: {'optimizer_name': 'adam', 'learning_rate': 0.0001, 'loss_type': 'combined', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 129.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:24<00:00,  4.87s/it, loss=0.444]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.65s/it, dice=0.000693, loss=0.533]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5015 | Val Loss: 0.4854 | Val Dice: 0.4722 | Val IoU: 0.3473\n",
            "Saved new best model with Dice: 0.4722\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.93s/it, loss=0.433]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.58s/it, dice=0.00112, loss=0.524]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4671 | Val Loss: 0.4853 | Val Dice: 0.4595 | Val IoU: 0.3310\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:29<00:00,  4.98s/it, loss=0.357]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.55s/it, dice=1, loss=0.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4541 | Val Loss: 0.4386 | Val Dice: 0.9324 | Val IoU: 0.8766\n",
            "Saved new best model with Dice: 0.9324\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:29<00:00,  5.00s/it, loss=0.498]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.62s/it, dice=0.00153, loss=0.516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4457 | Val Loss: 0.4414 | Val Dice: 0.6329 | Val IoU: 0.5252\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.94s/it, loss=0.377]\n",
            "Validation: 100%|| 11/11 [00:21<00:00,  1.95s/it, dice=0.167, loss=0.511]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4374 | Val Loss: 0.4313 | Val Dice: 0.7481 | Val IoU: 0.6811\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:30<00:00,  5.02s/it, loss=0.604]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=1, loss=0.505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4283 | Val Loss: 0.4126 | Val Dice: 0.9140 | Val IoU: 0.8492\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.95s/it, loss=0.425]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.66s/it, dice=0.00133, loss=0.509]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4258 | Val Loss: 0.4135 | Val Dice: 0.6747 | Val IoU: 0.5829\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.94s/it, loss=0.442]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=1, loss=0.514]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4170 | Val Loss: 0.4203 | Val Dice: 0.9113 | Val IoU: 0.8427\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.98s/it, loss=0.352]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.70s/it, dice=1, loss=0.514]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4087 | Val Loss: 0.4070 | Val Dice: 0.8948 | Val IoU: 0.8192\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.97s/it, loss=0.596]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.74s/it, dice=0.0435, loss=0.502]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4018 | Val Loss: 0.3996 | Val Dice: 0.7248 | Val IoU: 0.6518\n",
            "\n",
            "Training completed in 37.90 minutes\n",
            "Best validation Dice: 0.9324 at epoch 3\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 5 completed with best Dice: 0.9324 at epoch 3\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 6/7: {'optimizer_name': 'adam', 'learning_rate': 5e-05, 'loss_type': 'dice', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 131.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.816]\n",
            "Validation: 100%|| 11/11 [00:22<00:00,  2.06s/it, dice=4.75e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8740 | Val Loss: 0.8875 | Val Dice: 0.1127 | Val IoU: 0.0645\n",
            "Saved new best model with Dice: 0.1127\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.821]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.72s/it, dice=0.000204, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8183 | Val Loss: 0.7920 | Val Dice: 0.1480 | Val IoU: 0.0906\n",
            "Saved new best model with Dice: 0.1480\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:30<00:00,  5.01s/it, loss=0.797]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=7.22e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7896 | Val Loss: 0.7670 | Val Dice: 0.1432 | Val IoU: 0.0860\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:33<00:00,  5.08s/it, loss=0.782]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.58s/it, dice=0.000767, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7868 | Val Loss: 0.7777 | Val Dice: 0.1525 | Val IoU: 0.0924\n",
            "Saved new best model with Dice: 0.1525\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.97s/it, loss=0.672]\n",
            "Validation: 100%|| 11/11 [00:19<00:00,  1.81s/it, dice=2.91e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7627 | Val Loss: 0.7832 | Val Dice: 0.1194 | Val IoU: 0.0710\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.766]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=4.42e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7667 | Val Loss: 0.7830 | Val Dice: 0.1206 | Val IoU: 0.0718\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.95s/it, loss=0.68]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.67s/it, dice=5.5e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7507 | Val Loss: 0.7616 | Val Dice: 0.1398 | Val IoU: 0.0846\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.97s/it, loss=0.783]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000161, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7460 | Val Loss: 0.7960 | Val Dice: 0.1287 | Val IoU: 0.0779\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:29<00:00,  4.98s/it, loss=0.833]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.58s/it, dice=0.0256, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7475 | Val Loss: 0.7389 | Val Dice: 0.1567 | Val IoU: 0.0961\n",
            "Saved new best model with Dice: 0.1567\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.97s/it, loss=0.77]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.71s/it, dice=3.71e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7508 | Val Loss: 0.7806 | Val Dice: 0.1322 | Val IoU: 0.0797\n",
            "\n",
            "Training completed in 38.11 minutes\n",
            "Best validation Dice: 0.1567 at epoch 9\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 6 completed with best Dice: 0.1567 at epoch 9\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running experiment 7/7: {'optimizer_name': 'adam', 'learning_rate': 0.0005, 'loss_type': 'dice', 'num_epochs': 10}\n",
            "================================================================================\n",
            "\n",
            "Using device: cpu\n",
            "Found 7 training folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning folders: 100%|| 7/7 [00:00<00:00, 84.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 210 slices from 7 folders\n",
            "Training set: 168 slices\n",
            "Validation set: 42 slices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:35<00:00,  5.12s/it, loss=0.831]\n",
            "Validation: 100%|| 11/11 [00:16<00:00,  1.54s/it, dice=0.00219, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8468 | Val Loss: 0.8089 | Val Dice: 0.2355 | Val IoU: 0.1455\n",
            "Saved new best model with Dice: 0.2355\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.96s/it, loss=0.571]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=0.00329, loss=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7876 | Val Loss: 0.7872 | Val Dice: 0.4226 | Val IoU: 0.3140\n",
            "Saved new best model with Dice: 0.4226\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:30<00:00,  5.02s/it, loss=0.539]\n",
            "Validation: 100%|| 11/11 [00:16<00:00,  1.54s/it, dice=0.00076, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7600 | Val Loss: 0.8197 | Val Dice: 0.3391 | Val IoU: 0.2385\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:28<00:00,  4.95s/it, loss=0.518]\n",
            "Validation: 100%|| 11/11 [00:18<00:00,  1.71s/it, dice=4.35e-5, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7348 | Val Loss: 0.8530 | Val Dice: 0.1312 | Val IoU: 0.0772\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.94s/it, loss=0.603]\n",
            "Validation: 100%|| 11/11 [00:16<00:00,  1.54s/it, dice=1, loss=0.987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7346 | Val Loss: 0.7331 | Val Dice: 0.6087 | Val IoU: 0.4985\n",
            "Saved new best model with Dice: 0.6087\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:34<00:00,  5.10s/it, loss=0.632]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.56s/it, dice=1, loss=0.987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7008 | Val Loss: 0.7630 | Val Dice: 0.3784 | Val IoU: 0.2840\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.94s/it, loss=0.826]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.63s/it, dice=0.00196, loss=0.993]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7060 | Val Loss: 0.7412 | Val Dice: 0.4028 | Val IoU: 0.2826\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:27<00:00,  4.93s/it, loss=0.597]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.57s/it, dice=0.00026, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6803 | Val Loss: 0.7844 | Val Dice: 0.4643 | Val IoU: 0.3539\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:31<00:00,  5.04s/it, loss=0.4]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.55s/it, dice=0.000833, loss=0.996]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6882 | Val Loss: 0.7380 | Val Dice: 0.4681 | Val IoU: 0.3528\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 42/42 [03:29<00:00,  4.98s/it, loss=0.586]\n",
            "Validation: 100%|| 11/11 [00:17<00:00,  1.60s/it, dice=0.000252, loss=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6820 | Val Loss: 0.7555 | Val Dice: 0.3662 | Val IoU: 0.2534\n",
            "\n",
            "Training completed in 38.09 minutes\n",
            "Best validation Dice: 0.6087 at epoch 5\n",
            "Generating visualizations...\n",
            "\n",
            "Experiment 7 completed with best Dice: 0.6087 at epoch 5\n",
            "\n",
            "All experiments completed! Results saved to /content/drive/MyDrive/brain_tumor_model_output/experiments_20250425\n",
            "Multiple experiments complete!\n"
          ]
        }
      ]
    }
  ]
}